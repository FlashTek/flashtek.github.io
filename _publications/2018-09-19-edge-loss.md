---
title: "Faster Training of Mask R-CNN by Focusing on Instance Boundaries"
collection: publications
permalink: /publication/2018-09-19-edge-loss
excerpt: 'Speeding-up training of Mask R-CNN by introducing an auxillary loss.'
date: 2018-04-24
venue: 'Submitted to Computer Vision and Image Understanding'
paperurl: 'https://aip.scitation.org/doi/abs/10.1063/1.5022276'
citation: 'Zimmermann, R. S. and Siems, J. N. (2018). Faster Training of Mask R-CNN by Focusing on Instance Boundaries. arXiv preprint arXiv:1809.07069.'
---
We present an auxiliary task to Mask R-CNN, an instance segmentation network, which leads to faster training of the mask head. Our addition to Mask R-CNN is a new prediction head, the Edge Agreement Head, which is inspired by the way human annotators perform instance segmentation. Human annotators copy the contour of an object instance and only indirectly the occupied instance area. Hence, the edges of instance masks are particularly useful as they characterize the instance well. The Edge Agreement Head therefore encourages predicted masks to have similar image gradients to the groundtruth mask using edge detection filters. We provide a detailed survey of loss combinations and show improvements on the MS COCO Mask metrics compared to using no additional loss. Our approach marginally increases the model size and adds no additional trainable model variables. While the computational costs are increased slightly, the increment is negligible considering the high computational cost of the Mask R-CNN architecture. As the additional network head is only relevant during training, inference speed remains unchanged compared to Mask R-CNN. In a default Mask R-CNN setup, we achieve a training speed up of 29% and an overall improvement of 8.1% on the MS COCO metrics compared to the baseline.

[Download paper here](https://arxiv.org/abs/1809.07069)

Recommended citation: Zimmermann, R. S., & Parlitz, U. (2018). Observing spatio-temporal dynamics of excitable media using reservoir computing. Chaos: An Interdisciplinary Journal of Nonlinear Science, 28(4), 043118.